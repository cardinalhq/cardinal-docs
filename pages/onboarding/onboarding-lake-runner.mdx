# AWS

## Step 1

### EKS Cluster

The following are minimum requirements for running Cardinal Lake Runner in an EKS cluster in AWS


1. **Amazon EBS CSI Driver**


   1. [AWS help docs](https://docs.aws.amazon.com/eks/latest/userguide/ebs-csi.html)
   2. Used for persistent volumes required by the collector.
   3. Set a default storage class eg.

```bash
kubectl patch storageclass gp2 -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'
      ```
2. Node pool with minimum t3.medium instance types


   1. Given a minimal settings for the collector, t3.medium node instances can be used run end to end as a POC (not for production volume)

\

## Step 2

### S3 Bucket

When you choose to run Cardinal Lake Runner, you bring your own S3 bucket from AWS where your raw Otel data and CardinalHQ database files are stored.  Once the bucket is created you will create an IAM role which CardinalHQ services will assume.  This role will be given access to read and write to your bucket.  Finally, bucket notifications will be set to notify CardinalHQ of new files landing in the otel-raw prefix within your bucket.

The following steps are performed in the AWS console as well as the CardinalHQ web app on the [CardinalHQ Lake Runner configuration page](https://app.cardinalhq.io/configure/lakerunner) (use two browser tabs)


1. Create storage bucket and define it within CardinalHQ


   1. **AWS Console:**


      1. **Create a S3 bucket** in the AWS console in a CardinalHQ ***supported region***.


         1. Select a Cardinal supported region in the AWS console eg. US East Ohio : us-east-2
         2. Navigate to the AWS S3 console
         3. Provide an appropriate bucket name (copy the bucket name for the next step)
         4. Take the defaults and click the Create Bucket at the bottom of the page.
      2. **Setup bucket notifications**


         1. Click on the newly created bucket then navigate to the “Properties” tab
         2. Scroll down to “**Event notifications”**


            1. Click the “Create event notification”
            2. Enter  “CardinalHQ” for the event name
            3. Enter “otel-raw/” for the prefix
            4. Under **"Event types"** check "All object create events"
            5. Under **"Destination"** at the bottom of the page select "SQS queue"
            6. Select “Enter the SQS queue ARN”: (replace the BUCKET_REGION with the region selected above) eg. us-east-2

               ```javascript
               arn:aws:sqs:<BUCKET_REGION>:992382549843:cardinalhq-s3-notifications
               ```
            7. Select Save changes to save your notification settings.
   2. **CardinalHQ web app**:


      1. Click the  “+ Add Storage Bucket” button


         1. The Region dropdown will contain a list of ***supported regions***
      2. Enter the bucket name of the newly created S3 bucket.
   3. **AWS Console**:


      1. Create an IAM policy containing the permissions that CardinalHQ services will need to access your new bucket


         1. Navigate to **IAM** within the AWS console
         2. Click on **Policies** on the left-hand navigation pane
         3. Click the **Create Policy button** on the top-right
         4. Click the **JSON** button at the top of the Policy Editor
         5. Copy and past the “Permissions Policy” JSON block from the CardinalHQ web app’s “+ Add Storage Bucket” dialog into the Policy editor


            1. note: you will see the bucket name replaced in the JSON from the prior step
         6. Click Next
         7. Name the policy eg. “cardinalhq-services-permissions”
      2. Create an IAM role for use by CardinalHQ services


          1. Navigate to IAM within the AWS console
          2. Click roles on the left-hand navigation pane
          3. Click create Create Role button on the top-right
          4. Select the “Custom trust policy”
          5. Copy and paste the “trust relationship” JSON block from the CardinalHQ web app’s “+ Add Storage Bucket” dialog into the “**Custom trust policy” for your role.**
          6. Click Next
          7. In the Search box for “Permissions policies” search for the policy you created in the previous step. eg. “cardinalhq-services-permissions”
          8. Select/check the policy you created
          9. Click Next
         10. Provide an appropriate Role name eg. “CardinalHQServices” and optional description
         11. Create the role and then navigate to to the new role you created.
         12. Copy the ARN of the role you just created (we will use that in the next step)
   4. CardinalHQ web app:


      1. Paste the ARN of the IAM role which you created into the “Role ARN” textbox in the CardinalHQ web app’s “+ Add Storage Bucket” dialog
      2. Click the **Add bucket button** at the bottom of the “+ Add Storage Bucket” dialog

## Step 3

### **OpenTelemetry Collector**

In the CardinalHQ web app’s [CardinalHQ Lake Runner configuration page](https://app.cardinalhq.io/configure/lakerunner)


1. Click the “+ Add Collector” button
2. Provide a collector ID string


   1. This collector id is an identifier which can be used to segment your environment.  eg. “prod”,  “test”, etc.
3. Select the storage bucket you created in Step 2
4. Click the Add collector button

## Step 4

### Helm install of Otel collector

Prerequisite: There are a few ways to authorize your Otel collector to your bucket created above.  In the following steps we demonstrate a way of using an accessKey and secretKey from a user which has access to read/write to the bucket.  Please obtain those credentials now if you want to follow these steps.

In the CardinalHQ web app’s [CardinalHQ Lake Runner configuration page](https://app.cardinalhq.io/configure/lakerunner)


1. Looking at the “**OpenTelemetry Collectors” table**
2. Click on the ? Icon in the Actions column  to display  “Show install instructions…” for the collector you created in Step3
3. Create a local file on your computer called “values.yaml”
4. Paste the contents from the dialog’s yaml block into the values.yaml file on your computer and make the following changes:


   1. If you don’t have a default storage class set in your cluster, you may override this value pvc.storageClassName.
   2. A method of configuring your OTEL collector to have permissions to write to your S3 bucket is to provide access keys. (we recommend using EKS node identity or secrets in production)


      1. Obtain an accessKey and secretKey for an IAM user which has access to read and write to your bucket.
      2. Validate that the s3.authMode = “environment” in your “values.yaml” file
      3. Set your aws.accessKey and aws.secretKey to your keys
   3. Note: the collector deployment resources are defaulted appropriately for a small test environment
5. In a terminal/command prompt on your computer, change directories to the location where you created the values.yaml file and execute the helm install


   1. Create a “cardinalhq” namespace in your kubernetes cluster (validate you are pointing to your correct kubernetes cluster)

```bash
kubectl create namespace cardinalhq
      ```
   2. \
   3. Run the helm install using the values.yaml you created above

```bash
helm install --namespace cardinalhq cardinalhq-otel-collector oci://public.ecr.aws/cardinalhq.io/cardinalhq-otel-collector --values values.yaml
      ```
6. Your Otel collector pods should install and run

## Step 5

### Send data to your new Otel collector (Datadog agent install)

In order to see data flowing into your new collector, we can point a Datadog agent at the collector.

The following steps assume you have a fresh EKS cluster with no Datadog agent


1. In a terminal/command prompt on your computer


   1.  Create a datadog namespace

      ```bash
      kubectl create namespace datadog
      ```
   2. Create a yaml file called datadog.yaml and paste the following values. This config will point the DD agent to the collector you created in Step 4

```json
datadog:
  site: us3.datadoghq.com
  dd_url: "http://cardinalhq-otel-collector.cardinalhq.svc.cluster.local:8126"
  apiKey: demo-app-api-key


  checksCardinality: high
  dogstatsd:
    originDetection: true
    tagCardinality: high
    useHostPort: true

  logs:
    enabled: true
    containerCollectAll: true
    autoMultiLineDetection: true

  apm:
    socketEnabled: true
    portEnabled: true
    port: 8126

  tags:
    - cloud:aws
    - env:test
    - region:us-east-2
    - stack:cl
    - cluster:aws-test-us-east-2-cl


clusterAgent:
  replicas: 2
  createPodDisruptionBudget: true
  token: "IbQaAMilmcCJIgboCmtCRqCEoohsByON" # should be random per-installation

agents:
  useConfigMap: true

  image:
    tagSuffix: jmx

  customAgentConfig:
    apm_config:
      apm_dd_url: "http://cardinalhq-otel-collector.cardinalhq.svc.cluster.local:8126"

    logs_config:
      use_http: true
      logs_dd_url: "cardinalhq-otel-collector.cardinalhq.svc.cluster.local:8126"
      logs_no_ssl: true
      auto_multi_line_detection: true



      ```
   3. Add Datadog helm repo

```bash
helm repo add datadog https://helm.datadoghq.com
helm repo update
      ```
   4. Create the Datadog agent using helm

```bash
helm install ddagent datadog/datadog --values datadog.yaml --namespace datadog
      ```
2. After the Datadog agent starts up, it will begin sending data to your collector and then to CardinalHQ.


   1. You will see data flowing into your S3 bucket under the otel-raw/ prefix
   2. Since you successfully created a collector in Steps 2-5, you will also see data landing in a prefix db
   3. You should now see your data flowing into the CardinalHQ web app on the [https://app.cardinalhq.io/explore](https://app.cardinalhq.io/explore) page
